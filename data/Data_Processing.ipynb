{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing Subtitles Dataset and Creating JSON Objects: From Raw Text to Structured Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Data\n",
        "First we download a language dataset from the specified URL, rename it, and then extract its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mq2TZSnLe1q",
        "outputId": "48d879de-90af-49b0-bb38-72915464084c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-30 12:27:05--  https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/xml/fr.zip\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/xml/fr.zip [following]\n",
            "--2023-07-30 12:27:06--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/xml/fr.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6311975458 (5.9G) [application/zip]\n",
            "Saving to: ‘download.php?f=OpenSubtitles%2Fv2018%2Fxml%2Ffr.zip’\n",
            "\n",
            "download.php?f=Open 100%[===================>]   5.88G  21.0MB/s    in 4m 39s  \n",
            "\n",
            "2023-07-30 12:31:46 (21.6 MB/s) - ‘download.php?f=OpenSubtitles%2Fv2018%2Fxml%2Ffr.zip’ saved [6311975458/6311975458]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/xml/fr.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc4g6kojOr1u"
      },
      "outputs": [],
      "source": [
        "!mv /content/download.php?f=OpenSubtitles%2Fv2018%2Fxml%2Ffr.zip /content/fr.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D81-sseJPIsO"
      },
      "outputs": [],
      "source": [
        "!unzip fr.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating JSON Objects\n",
        "\n",
        "In this code cell, a Python function named `create_json_objects` is defined. It takes a list of lines as input and processes them to create JSON objects containing context, knowledge, and response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPI8E5a-uCsf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def create_json_objects(lines_list):\n",
        "    json_objects = []\n",
        "    context_lines = []\n",
        "    knowledge = \"\"\n",
        "\n",
        "    for line in lines_list:\n",
        "        # If we have less than 9 context lines, keep adding lines to the context\n",
        "        if len(context_lines) < 9:\n",
        "            context_lines.append(line.strip())\n",
        "        else:\n",
        "            # Create the JSON object for the current set of lines\n",
        "            response_line = line.strip()\n",
        "            if response_line:\n",
        "                json_object = {\n",
        "                    \"context\": context_lines.copy(),\n",
        "                    \"knowledge\": knowledge,\n",
        "                    \"response\": response_line\n",
        "                }\n",
        "                json_objects.append(json_object)\n",
        "\n",
        "            # Reset context_lines with the last 8 lines of the previous context\n",
        "            context_lines = context_lines[1:] + [line.strip()]\n",
        "\n",
        "    return json_objects\n",
        "\n",
        "def save_json_objects_to_file(json_objects, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(json_objects, file, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing XML Files and Creating JSON\n",
        "\n",
        "Here, XML files are processed, and JSON objects are created from the extracted data. We navigate through the XML structure, processe text data, and construct JSON objects. Progress is tracked using the tqdm library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzIX5luCy6z9",
        "outputId": "f4f86329-3dca-49aa-f585-f8869a5eb0c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  44%|████▍     | 56221/127204 [1:10:53<1:49:48, 10.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception occurred while processing file: /content/OpenSubtitles/xml/fr/2008/1031415/4528771.xml\n",
            "Exception details: not well-formed (invalid token): line 4135, column 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  79%|███████▉  | 101060/127204 [2:07:22<23:10, 18.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception occurred while processing file: /content/OpenSubtitles/xml/fr/2006/798028/4555239.xml\n",
            "Exception details: not well-formed (invalid token): line 3807, column 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 127204/127204 [2:40:48<00:00, 13.18it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import codecs\n",
        "\n",
        "folder_path = \"/content/OpenSubtitles/xml/fr\"\n",
        "output_folder = \"/content/new_dataset\"\n",
        "total_files = sum(len(files) for _, _, files in os.walk(folder_path))\n",
        "with tqdm(total=total_files, desc=\"Processing files\") as pbar:\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            try:\n",
        "                # Load XML file\n",
        "                file_path = os.path.join(root, file)\n",
        "                tree = ET.parse(file_path)\n",
        "                root_element = tree.getroot()\n",
        "\n",
        "                # # Remove time tags\n",
        "                # for time_tag in root_element.iter(\"time\"):\n",
        "                #     time_tag.clear()\n",
        "\n",
        "                # Process s tags\n",
        "                lines = []\n",
        "                for s_tag in root_element.iter(\"s\"):\n",
        "                    line = \" \".join(w_tag.text for w_tag in s_tag.findall(\"w\"))\n",
        "                    lines.append(line)\n",
        "\n",
        "                # Replace punctuation with just the punctuation mark in each line\n",
        "                lines = [line.replace(\" .\", \".\")\n",
        "                         .replace(\" ,\", \",\")\n",
        "                         .replace(\" !\", \"!\")\n",
        "                         .replace(\" ?\", \"?\")\n",
        "                         .replace(\" :\", \":\")\n",
        "                         .replace(\" ;\", \";\")\n",
        "                         .lstrip(\"- \") for line in lines]\n",
        "\n",
        "                # Write formatted text to a file with proper encoding\n",
        "                json_objects = create_json_objects(lines)\n",
        "                # new file name\n",
        "                directories = file_path.split(os.sep)\n",
        "                new_file = '-'.join(directories[-3:]).replace('.xml', '.json')\n",
        "                # new file path\n",
        "                output_path = os.path.join(output_folder, new_file)\n",
        "                # write to file\n",
        "                save_json_objects_to_file(json_objects, output_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Exception occurred while processing file: {file_path}\")\n",
        "                print(f\"Exception details: {str(e)}\")\n",
        "                # Remove the file if an exception is raised\n",
        "                # os.remove(file_path)\n",
        "\n",
        "            # Update the progress bar\n",
        "            pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSP8kifVXxnr",
        "outputId": "0abb1d5e-19f9-46d0-c3c7-c68cb5a7f6af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40G\t/content/new_dataset\n"
          ]
        }
      ],
      "source": [
        "!du -sh /content/new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning Up and Archiving\n",
        "\n",
        "In this part, we clean up temporary files, compresse the newly created JSON files into a ZIP archive, and copy the archive to a destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ceZF6nMasdk"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/OpenSubtitles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DiTHYoI5Wgvg",
        "outputId": "3d4448e6-6f30-4f07-e05e-d385647cd35c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/new_dataset.zip'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'your_folder_name' with the actual folder name containing the JSON files\n",
        "folder_path = '/content/new_dataset'\n",
        "output_zip_file = '/content/new_dataset.zip'\n",
        "\n",
        "shutil.make_archive(output_zip_file[:-4], 'zip', folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q9HzX-Fh2DpK",
        "outputId": "aec201f2-13b7-43fb-975e-78c1d2f268c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/new_dataset/new_dataset.zip'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_folder = '/content/new_dataset.zip'\n",
        "destination_folder = '/content/drive/MyDrive/new_dataset'\n",
        "\n",
        "shutil.copy(source_folder, destination_folder)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
