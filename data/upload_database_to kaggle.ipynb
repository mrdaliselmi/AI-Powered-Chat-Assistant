{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "AT_jyS9B0F3v",
        "outputId": "44e19b76-b17c-49e7-cf90-265c681061d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From:  https://drive.google.com/uc?id=1--jP1D4roOG88T2pjgNkskgMS64EFU6g\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 2.77G/2.77G [01:08<00:00, 40.2MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dataset.zip'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "url = \" https://drive.google.com/uc?id=1--jP1D4roOG88T2pjgNkskgMS64EFU6g\"\n",
        "out_path = \"dataset.zip\"\n",
        "gdown.download(url, out_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pW4LMJ0jQ95",
        "outputId": "fac156d0-d7e5-4e65-81cd-0b84dee8f563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDZHbf23wbgZ"
      },
      "outputs": [],
      "source": [
        "# !cp /content/drive/MyDrive/new_dataset/new_dataset.zip /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OUoTY6Tw5S3"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip -d /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vaG9eIYlx81A"
      },
      "outputs": [],
      "source": [
        "!rm -f /content/dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iQiyNh9x1US"
      },
      "source": [
        "## train-test-val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssvNJ93OxueB",
        "outputId": "6cb47834-74ab-4a97-f3a1-c0b676e4075e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Moving files to train: 100%|██████████| 89041/89041 [00:03<00:00, 25844.85it/s]\n",
            "Moving files to test: 100%|██████████| 25440/25440 [00:00<00:00, 34748.59it/s]\n",
            "Moving files to val: 100%|██████████| 12721/12721 [00:00<00:00, 34271.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files have been split and moved to the respective folders.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "# Set the paths for the source folder and destination folders\n",
        "source_folder = '/content/dataset'\n",
        "train_folder = '/content/dataset/train'\n",
        "test_folder = '/content/dataset/test'\n",
        "validation_folder = '/content/dataset/val'\n",
        "\n",
        "# Create destination folders if they don't exist\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "os.makedirs(validation_folder, exist_ok=True)\n",
        "\n",
        "# List all JSON files in the source folder\n",
        "json_files = glob.glob(os.path.join(source_folder, '*.json'))\n",
        "\n",
        "# Shuffle the list of JSON files randomly\n",
        "random.shuffle(json_files)\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.7\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.1\n",
        "\n",
        "# Calculate the number of files for each split\n",
        "num_files = len(json_files)\n",
        "num_train = int(train_ratio * num_files)\n",
        "num_test = int(test_ratio * num_files)\n",
        "num_validation = num_files - num_train - num_test\n",
        "\n",
        "# Assign files to different splits\n",
        "train_files = json_files[:num_train]\n",
        "test_files = json_files[num_train:num_train+num_test]\n",
        "validation_files = json_files[num_train+num_test:]\n",
        "\n",
        "# Move files to their respective folders with a progress bar\n",
        "for file_list, dest_folder in [(train_files, train_folder), (test_files, test_folder), (validation_files, validation_folder)]:\n",
        "    for file_path in tqdm(file_list, desc=f'Moving files to {os.path.basename(dest_folder)}'):\n",
        "        file_name = os.path.basename(file_path)\n",
        "        destination_path = os.path.join(dest_folder, file_name)\n",
        "        shutil.move(file_path, destination_path)\n",
        "\n",
        "print(\"Files have been split and moved to the respective folders.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bYJzlZq36T-",
        "outputId": "c4372ba3-7238-466f-8622-7e0737b40e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89041\n",
            "25440\n",
            "12721\n"
          ]
        }
      ],
      "source": [
        "!find /content/dataset/train -type f | wc -l\n",
        "!find /content/dataset/test -type f | wc -l\n",
        "!find /content/dataset/val -type f | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n77JsMJ_4a5e"
      },
      "outputs": [],
      "source": [
        "!zip -r dataset.zip /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHcVRXBg5Ghu",
        "outputId": "a41fb6a7-66ae-45cc-b3fd-0c1a8c405248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"daliselmi\",\"key\":\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FBiPWyFx-w5O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metadata = {\n",
        "  \"title\": \"French Conversations (from movie subtitles)\",\n",
        "  \"id\": \"daliselmi/French-Conversational-dataset\",\n",
        "  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
        "}\n",
        "\n",
        "json_object =json.dumps(metadata, indent=4)\n",
        "with open(\"/content/dataset/dataset-metadata.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8px4g04w-m8X",
        "outputId": "1d514e8d-834e-4fcc-cf25-c238ba56a4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting upload for file val.zip\n",
            "100% 263M/263M [00:03<00:00, 83.9MB/s]\n",
            "Upload successful: val.zip (263MB)\n",
            "Starting upload for file test.zip\n",
            "100% 528M/528M [00:05<00:00, 97.8MB/s]\n",
            "Upload successful: test.zip (528MB)\n",
            "Starting upload for file train.zip\n",
            "Request failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Will retry in 0.8 seconds\n",
            "100% 1.80G/1.80G [00:17<00:00, 111MB/s]\n",
            "Upload successful: train.zip (2GB)\n",
            "Your public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/daliselmi/French-Conversational-dataset\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets create --public --dir-mode zip -p /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhYaAtnSAWI2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
