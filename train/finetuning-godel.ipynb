{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install nltk\n!pip install python-dotenv\n!pip install scikit-learn\n!pip install sacrebleu\n!pip install rouge_score\n!pip install accelerate\n!pip install torch\n!pip install h5py\n!pip install datasketch\n!pip install revChatGPT\n!pip install evaluate\n!pip install openai","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport random\ndef concatenate_json_lists(directory_path, target_length, output_file_path):\n    concatenated_list = []\n    files_list = os.listdir(directory_path)\n    random.shuffle(files_list)\n    for filename in files_list:\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(directory_path, filename)\n            \n            with open(file_path, \"r\",encoding=\"utf-8\") as file:\n                data = json.load(file)\n                \n                if isinstance(data, list):\n                    concatenated_list.extend(data)\n                    \n                    if len(concatenated_list) >= target_length:\n                        break\n    \n    if len(concatenated_list) > 0:\n        concatenated_list = concatenated_list[:target_length]\n        \n        with open(output_file_path, \"w\",encoding=\"utf-8\") as output_file:\n            json.dump(concatenated_list, output_file, indent=2,ensure_ascii=False)\n            print(f\"Concatenated list written to {output_file_path}\")\n    else:\n        print(\"No data to concatenate or target length not reached.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:15.815483Z","iopub.execute_input":"2023-08-14T14:41:15.815953Z","iopub.status.idle":"2023-08-14T14:41:15.832188Z","shell.execute_reply.started":"2023-08-14T14:41:15.815911Z","shell.execute_reply":"2023-08-14T14:41:15.830246Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"directory_path = \"/kaggle/input/french-conversational-dataset/train\"\ntarget_length = 100000\noutput_file_path = \"/kaggle/working/train.json\"\nconcatenate_json_lists(directory_path, target_length, output_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:16.728369Z","iopub.execute_input":"2023-08-14T14:41:16.729556Z","iopub.status.idle":"2023-08-14T14:41:21.550818Z","shell.execute_reply.started":"2023-08-14T14:41:16.729513Z","shell.execute_reply":"2023-08-14T14:41:21.549806Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"Concatenated list written to /kaggle/working/train.json\n","output_type":"stream"}]},{"cell_type":"code","source":"directory_path = \"/kaggle/input/french-conversational-dataset/val\"\ntarget_length = int((1e5/6)*2)\noutput_file_path = \"/kaggle/working/validation.json\"\nconcatenate_json_lists(directory_path, target_length, output_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:21.553004Z","iopub.execute_input":"2023-08-14T14:41:21.553426Z","iopub.status.idle":"2023-08-14T14:41:23.498141Z","shell.execute_reply.started":"2023-08-14T14:41:21.553388Z","shell.execute_reply":"2023-08-14T14:41:23.497035Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Concatenated list written to /kaggle/working/validation.json\n","output_type":"stream"}]},{"cell_type":"code","source":"directory_path = \"/kaggle/input/french-conversational-dataset/test\"\ntarget_length = int((1e5/6)*2)\noutput_file_path = \"/kaggle/working/test.json\"\nconcatenate_json_lists(directory_path, target_length, output_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:23.499709Z","iopub.execute_input":"2023-08-14T14:41:23.500937Z","iopub.status.idle":"2023-08-14T14:41:25.328175Z","shell.execute_reply.started":"2023-08-14T14:41:23.500894Z","shell.execute_reply":"2023-08-14T14:41:25.327081Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"Concatenated list written to /kaggle/working/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir official_model","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:43:25.827679Z","iopub.execute_input":"2023-08-14T14:43:25.828556Z","iopub.status.idle":"2023-08-14T14:43:26.886178Z","shell.execute_reply.started":"2023-08-14T14:43:25.828511Z","shell.execute_reply":"2023-08-14T14:43:26.884613Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"CONTEXT_LEN = 1600\nNO_CONTEXT_LEN = 800","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:46.739933Z","iopub.execute_input":"2023-08-14T14:41:46.740361Z","iopub.status.idle":"2023-08-14T14:41:46.746036Z","shell.execute_reply.started":"2023-08-14T14:41:46.740323Z","shell.execute_reply":"2023-08-14T14:41:46.745090Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"%%file /kaggle/working/config.json\n{\n\"save_path\": \"/kaggle/working/official_model/\",\n\"length\": 1600,\n\"train\":\"/kaggle/working/train.json\",\n\"dev\":\"/kaggle/working/validation.json\",\n\"test\":\"/kaggle/working/test.json\",\n\"batch_size\": 4,\n\"batch_size_eval\":16,\n\"epoch\":10,\n\"lr\":2e-5,\n\"weight_decay\":1e-3,\n\"gpu\":-1,\n\"gradient_accumulation_steps\":32,\n\"optim\":\"adamw_torch\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:47.858202Z","iopub.execute_input":"2023-08-14T14:41:47.858669Z","iopub.status.idle":"2023-08-14T14:41:47.872306Z","shell.execute_reply.started":"2023-08-14T14:41:47.858633Z","shell.execute_reply":"2023-08-14T14:41:47.870951Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"%%file /kaggle/working/train.py\nfrom datasets.features.features import pa\nimport numpy as np\nimport os\nimport nltk\nimport argparse\nfrom datasets import load_dataset\nimport accelerate\nimport evaluate\nimport torch\nimport json\nnltk.download('punkt')\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments,TrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nparser = argparse.ArgumentParser(description='Transformation my dataset to group dataset type')\nparser.add_argument('--config', type=str,default=\"\",\n                    help='config file path ')\nargs = parser.parse_args()\n\nwith open(args.config, \"rb\") as f:\n  config = json.load(f)\n\nif len(list(os.listdir(config[\"save_path\"])))>0:\n  tokenizer = AutoTokenizer.from_pretrained(os.path.join(config[\"save_path\"],os.listdir(config[\"save_path\"])[0]))\n  model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(config[\"save_path\"],os.listdir(config[\"save_path\"])[0]))\n\nelse:\n  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n  model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n\n\ndef tokenize_function(examples):\n\n    instruction_k = f'Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge.'\n    instruction_nk = f'Instruction: given a dialog context, you need to response empathically.'\n    inputs = [f\"{instruction_k} [CONTEXT] {' EOS '.join(dialog)} [KNOWLEDGE] {knowledge}\" if knowledge != \"\" else\\\n              f\"{instruction_nk} [CONTEXT] {' EOS '.join(dialog)}\" for dialog, knowledge in zip(examples[\"context\"], examples[\"knowledge\"])]\n    targets = [ex for ex in examples[\"response\"]]\n    model_inputs = tokenizer(inputs, max_length=config[\"length\"], truncation=True)\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target= targets, max_length=config[\"length\"], truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    \n    return model_inputs\n\nclass f1:\n  def compute(self,predictions, references, type = 'marco'):\n    f1s =[]\n    precisions = []\n    recalls = []\n    for i in range(len(predictions)):\n      precision = 0\n      recall = 0\n      for j in \" \".join(predictions[i]).split():\n        if j in \" \".join(references[i][0]).split():\n          precision += 1\n      for j in \" \".join(references[i][0]).split():\n        if j in \" \".join(predictions[i]).split():\n          recall += 1\n      p = precision/(len(\" \".join(predictions[i]).split())+1)\n      r = recall/(len(\" \".join(references[i][0]).split())+1)\n      e = (1e-5)/(len(\" \".join(predictions[i]).split())+len(\" \".join(references[i][0]).split())+2)\n      precisions.append(p)\n      recalls.append(r)\n      f1s.append(2*p*r*(p+r)/((p+r)**2 +e**2))\n    if type == 'micro':\n      return {'f1': sum(f1s)/len(f1s)}\n    if type == 'marco':\n      e_a = (1e-5)/(len(precisions)+len(recalls))\n      p_a = sum(precisions)/len(precisions)\n      r_a = sum(recalls)/len(recalls)\n      return {'f1': 2*p_a*r_a*(p_a+r_a)/((p_a+r_a)**2 +e_a**2)}\n\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    if isinstance(predictions, tuple):\n        predictions = predictions[0].argmax(axis = -1)\n\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [[\"\\n\".join(nltk.sent_tokenize(label.strip()))] for label in decoded_labels]\n\n    # Metric\n    #rouge\n    result2 = metric2.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    #bleu\n    result1 = metric1.compute(predictions=decoded_preds, references=decoded_labels)\n    result2[\"bleu\"] = result1[\"bleu\"]\n    #meteor\n    result3 = metric3.compute(predictions=decoded_preds, references=decoded_labels)\n    result2[\"meteor\"] = result3[\"meteor\"]\n    #perplexity\n    result4 = metric4.compute(predictions=decoded_preds, model_id='gpt2')\n    result2[\"perplexity\"] = result4['mean_perplexity']\n    #f1\n    result5 = metric5.compute(predictions=decoded_preds, references=decoded_labels)\n    result2['f1'] = result5['f1']\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result2[\"gen_len\"] = np.mean(prediction_lens)\n    return {k: round(v, 4) for k, v in result2.items()}\n\n\n\nif __name__  == \"__main__\":\n    root = os.getcwd()\n    data_files = {\"train\": config[\"train\"], \"validation\":config[\"dev\"],\"test\":config[\"test\"]}\n    dataset = load_dataset(\"json\", data_files=data_files)\n    metric1 = evaluate.load(\"bleu\")\n    metric2 = evaluate.load(\"rouge\")\n    metric3 = evaluate.load(\"meteor\")\n    metric4 = evaluate.load(\"perplexity\", module_type=\"metric\")\n    metric5 = f1()\n\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n    training_args = Seq2SeqTrainingArguments(output_dir=config[\"save_path\"],\n                                  report_to=\"tensorboard\",\n                                  load_best_model_at_end = True,\n                                  save_strategy=\"epoch\",\n                                  evaluation_strategy=\"epoch\",\n                                  per_device_train_batch_size=config[\"batch_size\"],\n                                  per_device_eval_batch_size=config[\"batch_size_eval\"],\n                                  dataloader_num_workers=2,\n                                  fp16=True,\n                                  save_total_limit=1,\n                                  logging_strategy=\"epoch\",\n                                  predict_with_generate=True,\n                                  num_train_epochs=config[\"epoch\"],\n                                  learning_rate=config[\"lr\"],\n                                  weight_decay=config[\"weight_decay\"],\n\t\t  local_rank = config[\"gpu\"],\n\t\t  torch_compile = True,\n\t\t  optim = config[\"optim\"],\n\t\t  gradient_accumulation_steps = config[\"gradient_accumulation_steps\"]\n)\n    trainer =  Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"validation\"],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    trainer.save_model(config[\"save_path\"]+\"/model-v1.0.0\")\n    with open(config[\"save_path\"]+\"/test_results.txt\",\"w\") as f:\n         f.write(str(trainer.evaluate(tokenized_datasets[\"test\"])))","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:41:52.432823Z","iopub.execute_input":"2023-08-14T14:41:52.433199Z","iopub.status.idle":"2023-08-14T14:41:52.446140Z","shell.execute_reply.started":"2023-08-14T14:41:52.433167Z","shell.execute_reply":"2023-08-14T14:41:52.444662Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"# %%file /kaggle/working/transform.py\n# import json\n# import argparse\n\n# parser = argparse.ArgumentParser(description='Transformation my dataset to group dataset type')\n# parser.add_argument('--filepath', type=str,default=\"train.json\",\n#                     help='transformation file path ')\n# parser.add_argument('--save_file', type=str,default=\"train.json\",\n#                     help='transformation saving file path ')\n# parser.add_argument('--type', type=str,default=\"conv\",\n#                     help='transformation saving file path ')\n\n# args = parser.parse_args()\n\n# if __name__ == \"__main__\":\n#   filepath = args.filepath\n#   with open(filepath,\"rb\") as f:\n#     data =json.load(f)\n#   if args.type == \"conv\":\n#     transformed_train = []\n#     for da in data:\n#         data_point = {}\n#         for i, context in enumerate(da['context']):\n#             data_point[\"context\" +str(i)] =context\n#         data_point['response'] = da['response']\n#         transformed_train.append(data_point)\n#   elif args.type == \"dia\":\n#     transformed_train = []\n#     for da in data:\n#       try:\n#         transformed_train.append({\"dialog\": list(da.values())[:-1] , \"knowledge\": \"\", \"response\": da[\"response\"]})\n#       except Exception:\n#         continue\n# with open(args.save_file, \"w\", encoding=\"utf-8\") as f:\n#     print(len(transformed_train))\n#     for obj in transformed_train:\n#         json.dump(obj, f, ensure_ascii=False)\n#         f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-08-14T10:21:02.455249Z","iopub.execute_input":"2023-08-14T10:21:02.455676Z","iopub.status.idle":"2023-08-14T10:21:02.472165Z","shell.execute_reply.started":"2023-08-14T10:21:02.455639Z","shell.execute_reply":"2023-08-14T10:21:02.470665Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/transform.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%file /kaggle/working/transform.py\nimport json\nimport argparse\n\nparser = argparse.ArgumentParser(description='Transformation my dataset to group dataset type')\nparser.add_argument('--filepath', type=str,default=\"train.json\",\n                    help='transformation file path ')\nparser.add_argument('--save_file', type=str,default=\"train.json\",\n                    help='transformation saving file path ')\nparser.add_argument('--type', type=str,default=\"conv\",\n                    help='transformation saving file path ')\n\nargs = parser.parse_args()\n\nif __name__ == \"__main__\":\n  filepath = args.filepath\n  with open(filepath,\"rb\") as f:\n    data =json.load(f)\n#   if args.type == \"conv\":\n#     transformed_train = []\n#     for da in data:\n#         data_point = {}\n#         for i, context in enumerate(da['context']):\n#             data_point[\"context\" +str(i)] =context\n#         data_point['response'] = da['response']\n#         transformed_train.append(data_point)\n#   elif args.type == \"dia\":\n#     transformed_train = []\n#     for da in data:\n#       try:\n#         transformed_train.append({\"dialog\": list(da.values())[:-1] , \"knowledge\": \"\", \"response\": da[\"response\"]})\n#       except Exception:\n#         continue\nwith open(args.save_file, \"w\", encoding=\"utf-8\") as f:\n    print(len(data))\n    for obj in data:\n        json.dump(obj, f, ensure_ascii=False)\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:42:02.357223Z","iopub.execute_input":"2023-08-14T14:42:02.357688Z","iopub.status.idle":"2023-08-14T14:42:02.365606Z","shell.execute_reply.started":"2023-08-14T14:42:02.357651Z","shell.execute_reply":"2023-08-14T14:42:02.364432Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/transform.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/transform.py --filepath /kaggle/working/train.json --save_file /kaggle/working/train.json","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:42:05.244803Z","iopub.execute_input":"2023-08-14T14:42:05.245214Z","iopub.status.idle":"2023-08-14T14:42:10.411116Z","shell.execute_reply.started":"2023-08-14T14:42:05.245179Z","shell.execute_reply":"2023-08-14T14:42:10.409694Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stdout","text":"100000\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/transform.py --filepath /kaggle/working/test.json --save_file /kaggle/working/test.json","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:42:10.414542Z","iopub.execute_input":"2023-08-14T14:42:10.415381Z","iopub.status.idle":"2023-08-14T14:42:12.998543Z","shell.execute_reply.started":"2023-08-14T14:42:10.415338Z","shell.execute_reply":"2023-08-14T14:42:12.997074Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"33333\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/transform.py --filepath /kaggle/working/validation.json --save_file /kaggle/working/validation.json","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:42:13.001131Z","iopub.execute_input":"2023-08-14T14:42:13.001758Z","iopub.status.idle":"2023-08-14T14:42:15.520089Z","shell.execute_reply.started":"2023-08-14T14:42:13.001716Z","shell.execute_reply":"2023-08-14T14:42:15.518353Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"33333\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/train.py --config /kaggle/working/config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-14T14:43:38.057768Z","iopub.execute_input":"2023-08-14T14:43:38.059011Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-bcaa9012921f9394/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\nDownloading data files: 100%|███████████████████| 3/3 [00:00<00:00, 5329.48it/s]\nExtracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1022.92it/s]\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-bcaa9012921f9394/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 400.64it/s]\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n100%|█████████████████████████████████████████| 100/100 [00:54<00:00,  1.82ba/s]\n100%|███████████████████████████████████████████| 34/34 [00:18<00:00,  1.79ba/s]\n100%|███████████████████████████████████████████| 34/34 [00:18<00:00,  1.85ba/s]\nThe speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.\n  0%|                                                  | 0/3900 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n  2%|▊                                     | 81/3900 [19:43<15:35:47, 14.70s/it]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}